# ğŸ§  Learned Token Pruning: Patterns and Performance

## ğŸ“˜ Overview

This project investigates the effectiveness and impact of **Learned Token Pruning (LTP)** on transformer-based models, focusing on **efficiency**, **performance**, and **interpretability**. By pruning less informative tokens during fine-tuning, the goal is to reduce inference time while preserving classification accuracy.

We conduct experiments on short-text datasets like **AG News** and **IMDb Reviews**, analyzing token retention patterns and how pruning affects domain-specific vocabulary, performance, and semantic separability.

---

## ğŸ¯ Objectives

- Apply and evaluate LTP on fine-tuned RoBERTa models.
- Analyze the nature of retained tokens post-pruning.
- Evaluate the impact of pruning on model accuracy and interpretability.
- Study domain-specific token retention and performance impact.

---

## ğŸ§ª Methodology

### ğŸ”¹ Experiment 1: Model Training and Pruning
- **Baseline**: Fine-tune RoBERTa without pruning to establish benchmark metrics.
- **LTP Fine-Tuning**: Integrate the LTP mechanism into fine-tuning and dynamically prune tokens during training.
- **Threshold Adaptation**: Use soft pruning to allow layers to learn their pruning thresholds.

### ğŸ”¹ Experiment 2: Token Pattern Analysis
- **Extract Retained Tokens**: After pruning, gather all tokens that remain.
- **POS Tagging & Categorization**: Use `spaCy` to classify tokens into stopwords, adjectives, named entities, etc.
- **Frequency & Category Stats**: Compute retention frequencies per class.
- **Clustering**: Use t-SNE to visualize retained token distributions.

### ğŸ”¹ Experiment 3: Performance Evaluation
- **Metrics**: Accuracy and F1-score for baseline vs. pruned models.
- **Comparison**: Measure loss/gain in performance post-pruning.

### ğŸ”¹ Experiment 4: Domain-Specific Retention
- **Identify Keywords**: Manually select domain-specific keywords per dataset.
- **Retention Tracking**: Check if pruning retains or discards these terms.
- **Effect on Output**: Assess classification performance related to retained domain-specific vocabulary.

---

## ğŸ“Š Datasets

| Dataset     | Domain   | Description |
|-------------|----------|-------------|
| AG News     | General  | News articles in four categories: World, Sports, Business, Sci/Tech (~120 words each). |
| IMDb Reviews| Sentiment| Movie reviews labeled as positive or negative (~200 words each). |

---

## ğŸ“ Files and Their Uses

### ğŸ”¹ Model Training Pipelines
- `model_pipeline_agnews.ipynb` â€” Training pipeline using AG News dataset  
- `model_pipeline_imdb.ipynb` â€” Training pipeline using IMDb dataset

### ğŸ”¹ Trained Models (Post Phase 3)
- `ag_news_distilbert_hard/` â€” Final AG News model checkpoint - [Link here](https://drive.google.com/drive/folders/1jM8vA-b6g4q5cGAWh5AxTtjn6wG2kIJ6?usp=share_link)
- `imdb_distilbert_hard/` â€” Final IMDb model checkpoint - [Link here](https://drive.google.com/drive/folders/1oY8q8zXdxVc5AkxZEU6qRbZ5oAzg9UYA?usp=share_link)

### ğŸ”¹ Pruned CSV Outputs
Each CSV contains:
- Initial sentence
- Pruned sentence (final retained tokens)
- True and predicted labels
- POS tags of retained sentence

Files:
- `cleaned_agnews_pruned.csv`
- `cleaned_imdb_pruned.csv`

### ğŸ”¹ Results and Visualizations
- `results_agnews/` â€” Graphs and token analysis for AG News
- `results_imdb/` â€” Graphs and token analysis for IMDb

---

## ğŸ“ˆ Results Summary

### ğŸŸ  IMDb
- **Positive Sentiment**: Tokens like _â€œgreatâ€_, _â€œamazingâ€_, _â€œloveâ€_, _â€œrecommendâ€_ retained.
- **Negative Sentiment**: Words such as _â€œworstâ€_, _â€œawfulâ€_, _â€œboringâ€_, _â€œwasteâ€_ preserved.
- **POS Tags**: Adjectives and nouns dominate; content-bearing words are preserved more than grammatical ones.

### ğŸ”µ AG News
- **World**: geopolitics (e.g., _â€œpresidentâ€_, _â€œiraqâ€_, _â€œtroopsâ€_)
- **Sports**: temporal/action terms (_â€œfinalâ€_, _â€œseasonâ€_, _â€œwinâ€_)
- **Business**: market terms (_â€œstocksâ€_, _â€œprofitâ€_, _â€œsharesâ€_)
- **Sci/Tech**: tech nouns (_â€œgoogleâ€_, _â€œlinuxâ€_, _â€œspaceâ€_)

### ğŸ” t-SNE Clustering
- IMDb: Clear positive vs. negative separation; denser positive clusters.
- AG News: Coherent clusters for each topic, showing strong domain-specific token retention.

---

## ğŸ“Œ Key Insights

- Pruning preserves **semantically relevant, high-impact tokens**.
- POS distribution favors **adjectives, nouns, and verbs** that contribute meaning.
- Domain-specific keywords are **effectively retained**, supporting classification.
- LTP leads to significant **token reduction** with **minimal accuracy drop**.


---

## ğŸ§  Expected Outcomes

- **Token Pruning Patterns**: Insight into which token types are preserved vs. dropped.
- **Performance Trade-offs**: Measure LTPâ€™s effect on classification metrics.
- **Interpretability**: Clarity on how models make decisions post-pruning.

---

## ğŸ“¦ Requirements

- Python 3.8+
- PyTorch
- Transformers (Hugging Face)
- spaCy
- scikit-learn
- Matplotlib, Seaborn

Install via:

```bash
pip install -r requirements.txt




